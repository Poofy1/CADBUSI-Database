{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ad32ac-07f7-4e9b-8e2c-25f96dfc28fb",
   "metadata": {},
   "source": [
    "## To Do (5/9/23)\n",
    "\n",
    "* clean up description processing code and wrap it all into a single function\n",
    "    * works similarly to image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbf9b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tristan\\anaconda3\\envs\\Pytorch_1-12-1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import usImageProc as uip\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import largestinteriorrectangle as lir\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "import re\n",
    "\n",
    "import easyocr\n",
    "# configure easyocr reader\n",
    "reader = easyocr.Reader(['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9505fb99-3c14-40df-be69-bff43939359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "description_kw = ['breast','lt','long','rt','trans','area','palpated','axilla','areolar','radial','marked','supraclavicular','oblique','contrast']\n",
    "description_kw_expand= ['cm','fn','breast','lt','long','rt','trans','area',\n",
    "                        'palpated','axilla','areolar','radial','marked',\n",
    "                        'supraclavicular','oblique','contrast','retroareolar',\n",
    "                        'harmonics','axillary','subareolar','nipple','anti', \n",
    "                        'periareolar','subclavicular']\n",
    "description_kw_contract = ['retro areolar', \n",
    "                           'sub areoloar', \n",
    "                           'peri areolar',\n",
    "                           'anti -rad']\n",
    "description_kw_sub = {'scm':'5 cm', \n",
    "                      'anti radial':'anti-rad', \n",
    "                      'axillary':'axilla', \n",
    "                      'axlla':'axilla',\n",
    "                      'subclavcular':'subclavicular'}\n",
    "\n",
    "description_labels_dict = {\n",
    "    'area':{'breast':['breast'],\n",
    "            'axilla':['axilla'],\n",
    "            'supraclavicular':['superclavicular','supraclavicular'],\n",
    "            'subclavicular':['subclavicular','subclavcular']},\n",
    "    'laterality':{'left':['lt','left'],\n",
    "                  'right':['rt','right']},\n",
    "    'orientation':{'long':['long'],\n",
    "                    'trans':['trans'],\n",
    "                    'anti-radial':['anti-rad','anti-radial'],\n",
    "                    'radial':['radial'],\n",
    "                    'oblique':['oblique']}\n",
    "}\n",
    "\n",
    "# image id followed by dictionary of corrections to apply\n",
    "corrections = { 4094:{'description':'long lt breast 10.00 scm fn area palpated', 'area':'breast'} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c5f1c4-3ebe-413b-9af5-12f82c46f56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note these numbers refer to the filenames, e.g.\n",
    "# 2 → 000002_cropped.png\n",
    "# subtract 1 from each to get the index in the alphabetized list\n",
    "\n",
    "sectors = [2,11,12,94,100,287,291,522,523,525,526,\n",
    "          527,528,530,531,533,536,544,559,635,637,\n",
    "          638,639,640,641,645,646,892,916,917,918,\n",
    "          919,920,972,973,978,983,984,1140,1146,1147,\n",
    "          1150,1498,1553,1555,1556,1557,1710,1711,\n",
    "           1712,1713,1714,1715,1716,1717,1718,1856,\n",
    "          1857,1861,1862,1863,1864,1973,1978,1979,\n",
    "          1982,1984,1985,1987,1988,1992,1995,1998]\n",
    "traps = [ ]\n",
    "len(sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0781a19f-e733-4eab-bbe1-a8d97500ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for text utilities\n",
    "import re\n",
    "\n",
    "# Helper functions for text processing - mostly used to extract description from image\n",
    "\n",
    "def contains_substring(input_string, substring_list):\n",
    "    for substring in substring_list:\n",
    "        if substring in input_string:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_digit(input_string):\n",
    "    pattern = re.compile(r'\\d') # Compile a regular expression pattern to match digits\n",
    "    return bool(pattern.search(input_string)) # Return True if a match is found, False otherwise\n",
    "\n",
    "def text_freq_df_column( df, col = 'description'):\n",
    "    \"\"\"Compute frequencies of words in column of strings.  Not case sensitive.\n",
    "    \n",
    "    Helps to identify keywords and also mispellings\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas dataframe \n",
    "        col: column of strings for frequency analysis (defaults to 'description')\n",
    "    \n",
    "    Returns:\n",
    "        counts:  pd series of counts indexed by words in descending order of frequency\n",
    "        \n",
    "    Example:\n",
    "        db = pd.read_csv('database_total.csv')\n",
    "        db = db.fillna('')\n",
    "        counts = text_freq_df_column(db)\n",
    "        counts[0:60]\n",
    "    \"\"\"\n",
    "\n",
    "    soup = []\n",
    "    for d in df[col]:\n",
    "        if d is not None:\n",
    "            split_soup = d.split(' ')\n",
    "            for s in split_soup:\n",
    "                s = s.lower()\n",
    "                s = s.replace('/','')\n",
    "                if s !='' and not has_digit(s):\n",
    "                    soup.append(s)\n",
    "    print(len(soup))\n",
    "    counts = pd.Series(soup).value_counts()\n",
    "    return counts\n",
    "\n",
    "def pad_substrings_with_spaces(substrings, input_str):\n",
    "    # Iterate over each substring in the list\n",
    "    for substring in substrings:\n",
    "        # Replace each occurrence of the substring with the same substring padded with spaces\n",
    "        input_str = input_str.replace(substring, f\" {substring} \")\n",
    "\n",
    "    # remove duplicate spaces    \n",
    "    words = input_str.split()\n",
    "    input_str = ' '.join(words)\n",
    "    \n",
    "    # Return the string\n",
    "    return input_str\n",
    "\n",
    "def clean_text(input_str, sub_dict, kw_expand, kw_contract):\n",
    "    \"\"\"Process input string and add spaces around keywords, substitute for common OCR mistakes\n",
    "    \n",
    "    Args:\n",
    "        input_str: string to be processed\n",
    "        kw_list:  any word in this list will be searched and padded with spaces\n",
    "        repair_dict: keys are substrings that will be replaced by their corresponding values\n",
    "    \n",
    "    Returns:\n",
    "        output_str: repaired string\n",
    "    \"\"\"\n",
    "    \n",
    "    # first make substitutions\n",
    "    for k in sub_dict.keys():\n",
    "        input_str = input_str.replace( k, sub_dict[k] )\n",
    "        \n",
    "    # now add spaces around all substrings in kw_expand\n",
    "    for substring in kw_expand:\n",
    "        input_str = input_str.replace( substring, f\" {substring} \" )\n",
    "        \n",
    "    # remove duplicate spaces\n",
    "    words = input_str.split()\n",
    "    input_str = ' '.join(words)\n",
    "    \n",
    "    # remove spaces from words in kw_contract\n",
    "    for substring in kw_contract:\n",
    "        input_str = input_str.replace( substring, substring.replace(' ','') )\n",
    "        \n",
    "    return input_str\n",
    "\n",
    "def clean_text_df( df, sub_dict, kw_expand, kw_contract, col = 'description'):\n",
    "    df[col] = df[col].apply(clean_text, args = (sub_dict, kw_expand, kw_contract) )\n",
    "\n",
    "def label_parser(x, label_dict={}):\n",
    "    for k in label_dict.keys():\n",
    "        labels = label_dict[k]\n",
    "        if contains_substring(x,labels):\n",
    "            return k\n",
    "    return 'unknown'\n",
    "\n",
    "def find_time_substring(text):\n",
    "    # Regular expression to match time substrings of the form HH:MM or HH.MM\n",
    "    # does not need to be have blank spaces\n",
    "    pattern = r'\\d{1,2}[:.]\\d{2}'\n",
    "    \n",
    "    # Find all matches in the input text\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    if len(matches)==0:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        # Return only the first match\n",
    "        time = matches[0].replace('.',':')\n",
    "        return time\n",
    "    \n",
    "def find_cm_substring(input_str):\n",
    "    \"\"\"Find first substring of the form #cm or # cm or #-#cm or #-# cm, not case sensitive\n",
    "    \n",
    "    Args:\n",
    "        input_str:  string\n",
    "        \n",
    "    Returns:\n",
    "        list of matched substrings\n",
    "    \"\"\"\n",
    "    # Regular expression to match s\n",
    "    pattern = r'\\d+(-\\d+)?\\s*cm'\n",
    "    \n",
    "    input_str = input_str.lower()\n",
    "    input_str = input_str.replace(\"scm\",\"5cm\") #easyocr sometimes misreads 5cm as scm\n",
    "    \n",
    "    # Find all matches in the input string\n",
    "    matches = re.finditer(pattern, input_str)\n",
    "    \n",
    "    # get list of matches\n",
    "    list_of_matches = [m.group() for m in matches]\n",
    "    \n",
    "    if len(list_of_matches)==0:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return list_of_matches[0]\n",
    "    \n",
    "def extract_descript_features( input_str, labels_dict ):\n",
    "    \n",
    "    output_dict = {}\n",
    "    for feature in labels_dict.keys():\n",
    "        levels_dict = labels_dict[feature]\n",
    "        output_dict[feature] = label_parser( input_str, levels_dict)\n",
    "\n",
    "    output_dict['clock_pos'] = find_time_substring(input_str)\n",
    "    output_dict['nipple_dist'] = find_cm_substring(input_str)\n",
    "    \n",
    "    return output_dict\n",
    "    \n",
    "def extract_descript_features_df(df, labels_dict, col = 'description'):\n",
    "\n",
    "    # first extract simple text features\n",
    "    for feature in labels_dict.keys():\n",
    "        levels_dict = labels_dict[feature]\n",
    "        df[feature] = df[col].apply( label_parser, label_dict = levels_dict )\n",
    "    \n",
    "    # extract clock_position\n",
    "    df['clock_pos'] = df[col].apply( find_time_substring )\n",
    "    \n",
    "    # extract nipple_dist\n",
    "    df['nipple_dist'] = df[col].apply( find_cm_substring )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6869de49-bddf-4482-947a-5e0c660d1f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 'breast',\n",
       " 'laterality': 'right',\n",
       " 'orientation': 'long',\n",
       " 'clock_pos': '8:00',\n",
       " 'nipple_dist': '5cm'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = 'long rt breast 8.00 5cm fn'\n",
    "dict = extract_descript_features( description, description_labels_dict)\n",
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b136ab56-5e54-461b-9463-559f21d49246",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'database_total.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m db \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdatabase_total.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m db \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39miloc[:\u001b[39m100\u001b[39m]\n\u001b[0;32m      3\u001b[0m db2 \u001b[39m=\u001b[39m extract_descript_features_df( db, description_labels_dict )\n",
      "File \u001b[1;32mc:\\Users\\Tristan\\anaconda3\\envs\\Pytorch_1-12-1\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tristan\\anaconda3\\envs\\Pytorch_1-12-1\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tristan\\anaconda3\\envs\\Pytorch_1-12-1\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Tristan\\anaconda3\\envs\\Pytorch_1-12-1\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Tristan\\anaconda3\\envs\\Pytorch_1-12-1\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Tristan\\anaconda3\\envs\\Pytorch_1-12-1\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Tristan\\anaconda3\\envs\\Pytorch_1-12-1\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'database_total.csv'"
     ]
    }
   ],
   "source": [
    "db = pd.read_csv('database_total.csv')\n",
    "db = db.iloc[:100]\n",
    "db2 = extract_descript_features_df( db, description_labels_dict )\n",
    "db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6571ab7-6dbe-4244-a5b1-e0da314861f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4098/4098 [29:50<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "image_folder_path = r\"C:/Users/jbaggett/image\"\n",
    "proc_images_folder = r\"C:/Users/jbaggett/proc_image\"\n",
    "\n",
    "input_file = r\"database.csv\"\n",
    "output_file = r\"database_testing.csv\"\n",
    "\n",
    "# processing configuration\n",
    "debug = False\n",
    "write_images = True\n",
    "display_images = False\n",
    "\n",
    "# open database and get filenames to be processed\n",
    "db_in = pd.read_csv(input_file)\n",
    "files = db_in['filename']\n",
    "image_numbers = np.arange(len(files))\n",
    "\n",
    "# open or create output database\n",
    "import os.path\n",
    "check_db_out = os.path.isfile(output_file)\n",
    "if check_db_out:\n",
    "    db_out = pd.read_csv(output_file)\n",
    "else:\n",
    "    db_out = db_in.copy()\n",
    "    new_features = ['processed','crop_x', 'crop_y', 'crop_w', 'crop_h', 'description', 'size', 'sector_detected', 'darkness','area','laterality','orientation','clock_pos','nipple_dist']\n",
    "    for nf in new_features:\n",
    "        db_out[nf] = None\n",
    "    db_out['processed'] = False\n",
    "                \n",
    "\n",
    "for i in tqdm(image_numbers):\n",
    "    sleep(0.01)\n",
    "    if not db_out['processed'][i]:\n",
    "        file_name = db_in['filename'][i]\n",
    "        us_x = min(db_in['us_x0'][i],0)\n",
    "        us_y = db_in['us_y0'][i]\n",
    "        us_w = db_in['us_x1'][i]-us_x\n",
    "        us_h = db_in['us_y1'][i]-us_y\n",
    "        rect_us = (us_x, us_y, us_w, us_h)\n",
    "        #print('rect_us: ', rect_us)\n",
    "        # Check if the file is an image\n",
    "        if file_name.endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            # Construct the full path to the image file\n",
    "\n",
    "            if debug:\n",
    "                print('Processing: ', file_name )\n",
    "\n",
    "            full_filename = os.path.join(image_folder_path, file_name)\n",
    "            image_out_path = os.path.join(proc_images_folder, file_name)\n",
    "\n",
    "            # Open the image file and store it in an image object\n",
    "            img = Image.open(full_filename)\n",
    "\n",
    "            # recast image as numpy array\n",
    "            img = np.array(img)\n",
    "            img_orig = img.copy()\n",
    "\n",
    "            img_dict = uip.img_processor(img, reader, \n",
    "                                         rect_US = rect_us,\n",
    "                                         kw_list = description_kw)\n",
    "            if debug: \n",
    "                print(img_dict)\n",
    "                print('Processing Complete: ', file_name)\n",
    "\n",
    "            # insert into total database\n",
    "            new_features = ['crop_x', 'crop_y', 'crop_w', 'crop_h', 'description', 'size', 'is_sector', 'darkness']\n",
    "            crop_x, crop_y, crop_w, crop_h = img_dict['rect_crop']\n",
    "            description = img_dict['text_description']\n",
    "            db_out.loc[i,'crop_x'] = crop_x\n",
    "            db_out.loc[i,'crop_y'] = crop_y\n",
    "            db_out.loc[i,'crop_w'] = crop_w\n",
    "            db_out.loc[i,'crop_h'] = crop_h\n",
    "            db_out.loc[i,'description'] = description\n",
    "            db_out.loc[i,'size'] = img_dict['text_size']\n",
    "            db_out.loc[i,'sector_detected'] = img_dict['sector_detected']\n",
    "            db_out.loc[i,'processed'] = True\n",
    "            db_out.loc[i,'darkness'] = img_dict['darkness']\n",
    "            if len(description)>0:\n",
    "                feature_dict = extract_descript_features( description, description_labels_dict )\n",
    "                display_str = ''\n",
    "                for feature in feature_dict.keys():\n",
    "                    db_out.loc[i,feature] = feature_dict[feature]\n",
    "                    display_str = display_str + feature_dict[feature] + ' '\n",
    "            else:\n",
    "                display_str = ''\n",
    "\n",
    "            if write_images or display_images: # add description and crop region to image\n",
    "                img_orig = uip.add_rect(img_orig, img_dict['rect_crop'])\n",
    "                img_orig = uip.add_text(img_orig, display_str)\n",
    "\n",
    "            if write_images:\n",
    "                cv2.imwrite(image_out_path,img_orig)\n",
    "            if display_images:\n",
    "                img2 = img_orig.copy()\n",
    "                img2 = uip.add_rect(img2, img_dict['rect_machine'])\n",
    "                img2 = uip.add_rect(img2, img_dict['rect_description'])\n",
    "                img2 = uip.add_rect(img2, img_dict['rect_colorbar'])\n",
    "                if len(img_dict['rect_sizebox'])>0:\n",
    "                    img2 = uip.add_rect(img2, img_dict['rect_sizebox'])\n",
    "                    \n",
    "                fig, (ax1,ax2) = plt.subplots(1,2,figsize=(20, 15)) \n",
    "\n",
    "                ax1.imshow(img_orig,cmap='gray')   \n",
    "                ax2.imshow(img2,cmap='gray')\n",
    "                fig.show()                \n",
    "                \n",
    "db_out.to_csv(output_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceaa02bf-8844-456c-8809-8840f7f0cf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "330a5263-0012-47df-a1c2-59b651ff999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "breast             3563\n",
       "axilla              486\n",
       "unknown              29\n",
       "supraclavicular      16\n",
       "subclavicular         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_freq_df_column( db_out, col = 'area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "448954d5-ccf2-4cf4-990f-98a2a47b5eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    13\n",
       "13    14\n",
       "14    15\n",
       "15    16\n",
       "16    17\n",
       "17    18\n",
       "18    19\n",
       "19    20\n",
       "20    21\n",
       "21    22\n",
       "22    23\n",
       "23    24\n",
       "24    25\n",
       "25    26\n",
       "26    27\n",
       "27    28\n",
       "Name: image_id, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_out[ db_out['patient_id']==4]['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9be53ea6-8e60-4611-a6fc-c766c6228e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# database utilities\n",
    "\n",
    "def db_filters( db_in, db_out = None, only_breast = True, only_gray = False, only_calipers = False, max_darkness = 50):\n",
    "    # db_in is the name of the csv file containing our database\n",
    "    # returns a dataframe with filterrs applied and optionally writes to db_out\n",
    "    pass\n",
    "\n",
    "def fetch_index_for_patient_id( id, db, only_gray = False, only_calipers = False ):\n",
    "    # id is a patient id number that should be listed in database\n",
    "    # only_gray = True → return only monochrome files (not doppler)\n",
    "    # only_calipers = True → return only files that include calipers\n",
    "    # returns list of indices\n",
    "    \n",
    "    if id in db['patient_id'].tolist():\n",
    "         indices= db.index[db['patient_id']==id].tolist()\n",
    "    else:\n",
    "        indices = []\n",
    "    return indices\n",
    "\n",
    "fetch_index_for_patient_id( 4, db_out )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "faa2cd89-e139-4f45-a8e7-351175304f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 361/361 [21:34<00:00,  3.59s/it]\n"
     ]
    }
   ],
   "source": [
    "image_folder_path = r\"C:/Users/jbaggett/image\"\n",
    "\n",
    "\n",
    "def find_nearest_images( db, patient_id):\n",
    "\n",
    "    idx = np.array(fetch_index_for_patient_id(patient_id, db_out))\n",
    "    num_images = len(idx)\n",
    "    result = {}\n",
    "\n",
    "    for j,c in enumerate(idx):\n",
    "        x = db_out.loc[c]['crop_x']\n",
    "        y = db_out.loc[c]['crop_y']\n",
    "        w = db_out.loc[c]['crop_w']\n",
    "        h = db_out.loc[c]['crop_h']\n",
    "        img_stack = np.zeros((num_images,w*h))\n",
    "        root_image = np.where(calipers==c)\n",
    "        for i,image_id in enumerate(idx):\n",
    "            file_name = db.loc[image_id]['filename']\n",
    "            full_filename = os.path.join(image_folder_path, file_name)\n",
    "            img = Image.open(full_filename)\n",
    "            img = np.array(img)\n",
    "            img,_ = uip.make_grayscale(img)\n",
    "            img = img[y:y+h,x:x+w]\n",
    "            img = img.flatten()\n",
    "            img_stack[i,:] = img\n",
    "        img_stack = np.abs(img_stack - img_stack[j,:])\n",
    "        img_stack = np.mean( img_stack, axis=1 ) \n",
    "        img_stack[j] = 1000\n",
    "        sister_image = np.argmin(img_stack)\n",
    "        distance = img_stack[sister_image]\n",
    "        result[c] = {'filename': db.loc[c]['filename'],\n",
    "                     'sister_filename':db.loc[ idx[sister_image]]['filename'],\n",
    "                     'distance': distance}\n",
    "    return result\n",
    "\n",
    "\n",
    "input_file = r'database_total.csv'\n",
    "output_file = r'datbase_total_v2.csv'\n",
    "\n",
    "db_in = pd.read_csv(input_file)\n",
    "patient_ids = db_in['patient_id'].unique()\n",
    "        \n",
    "db_out = db_in.copy()\n",
    "db_out['closest_fn']=''\n",
    "db_out['distance'] = -1\n",
    "\n",
    "for pid in tqdm(patient_ids):\n",
    "    result = find_nearest_images(db_in, 4)\n",
    "    idxs = result.keys()\n",
    "    for i in idxs:\n",
    "        db_out.loc[i,'closest_fn'] = result[i]['sister_filename']\n",
    "        db_out.loc[i,'distance'] = result[i]['distance']\n",
    "        \n",
    "db_out.to_csv(output_file,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e4872a46-0686-49e3-a16b-fa0dfd5667ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[0 0 0]\n",
      " [3 3 3]\n",
      " [6 6 6]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([ [1,2,3],[4,5,6],[7,8,9] ] )\n",
    "print(x)\n",
    "print(x-x[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6baa3204-c199-409e-91dd-ab351f717d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         2.82842712 5.65685425]\n",
      " [2.82842712 0.         2.82842712]\n",
      " [5.65685425 2.82842712 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def compute_distance_matrix(arr):\n",
    "    \"\"\"\n",
    "    Computes a distance matrix for differences along the first dimension of a numpy array.\n",
    "    \"\"\"\n",
    "    return cdist(arr, arr, 'euclidean')\n",
    "\n",
    "# Define a numpy array\n",
    "arr = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Compute the distance matrix\n",
    "dist_matrix = compute_distance_matrix(arr)\n",
    "\n",
    "# Print the result\n",
    "print(dist_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0b859462-78a8-4791-9534-89deb89e1b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a 3 cm test of long lt'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dacd190-1401-41b3-b83c-ead5599a79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this up\n",
    "# the size box seems to always have the horizontal position\n",
    "# but the vertical coordinates can vary\n",
    "#\n",
    "# read the text in that region, if any, using easyocr\n",
    "# use the text coordinates to estimate the height of the rectangle\n",
    "# may not use erosion and dilation at all\n",
    "def get_box_rect(img, rect = (720,520,160,200), thresh = 40):\n",
    "    # img is HxW or HxWx3 image in numpy array\n",
    "    # look for box in lower right corner and return coordinates\n",
    "    # return tuple (x,y,w,h) if box detected, otherwise empty tuple ()\n",
    "\n",
    "    img_gray, is_color = make_grayscale(img)\n",
    "    img_bw = make_mask(img_gray, thresh = thresh)\n",
    "\n",
    "    # blackout everything not in box region (this won't generalize cuz the approx box loc is hardwired)\n",
    "    H,W = img_bw.shape\n",
    "    x,y,w,h = rect\n",
    "    rect = (x,y,min(W-x,w),min(H-y,h)) # truncate if box extends too far down or right\n",
    "    img_bw = blackout_rectangle_exterior(img_bw,rect)\n",
    "    \n",
    "    # dilate a bit to make sure box is closed\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    img_dilated = cv2.dilate(img_bw,kernel,iterations=1)\n",
    "    \n",
    "    # get contours and find largest, if its big enough get box coordinates\n",
    "    contours, hierarchy = cv2.findContours(img_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    area = 0\n",
    "    size_strings = []\n",
    "    if len(contours) > 0:\n",
    "        c = max(contours,key=cv2.contourArea)\n",
    "        area = cv2.contourArea(c)\n",
    "        if area >= 1000:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            img_cleaned = blackout_rectangle_exterior(img, (x,y,w,h) )\n",
    "            result = reader.readtext(img_cleaned,paragraph=False)\n",
    "            size_box_found = len(result)>0\n",
    "            if size_box_found:\n",
    "                size_strings = [r[1] for r in result]\n",
    "\n",
    "    if (area >= 1000) and size_box_found:\n",
    "        box_rect = (x,y,w,h)\n",
    "    else:\n",
    "        box_rect = None\n",
    "        \n",
    "    return box_rect, size_strings\n",
    "\n",
    "def get_text_box(img, thresh = 170, rect = (22,500,811,220), solidify = True, pos0 = (409,655)):\n",
    "    # img is HxW or HxWx3 image in numpy array\n",
    "    # look for text at bottom of image\n",
    "    # return tuple (x,y,w,h) of rectangle containing text and text string\n",
    "    # this works best if the text has a clear separation from the \"size box\" in the lower right corner\n",
    "    # remove the \"size box\" before text extraction\n",
    "    # pos0 is used as reference point, the closest \"paragraph\" to the ref point will be returned\n",
    "    #     if multiple strings are found in the rectangle\n",
    "\n",
    "    # if image has multiple channels convert it to grayscale (could probably improve this)\n",
    "    if len(img.shape)>2:\n",
    "        img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        img_gray = img.copy()\n",
    "    \n",
    "    # black out everything outside of text region\n",
    "    H,W = img_gray.shape\n",
    "    img_gray = blackout_rectangle_exterior( img_gray, rect)\n",
    "                                                                                \n",
    "    # convert to black and white to increase contrast\n",
    "    threshold = thresh\n",
    "    th,img_bw = cv2.threshold(img_gray,threshold,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "    # new approach 4/12/23\n",
    "    #\n",
    "    # use easyocr as first pass and get longest string and coordinates of box\n",
    "    # use tesseract to extract string from the box provided by easyocr\n",
    "    img_focused = blackout_rectangle_exterior( img, rect)\n",
    "    result = reader.readtext(img_focused,paragraph=True)\n",
    "    #print('Easyocr results: ',result)\n",
    "    lengths = [ len(r[1]) for r in result ]\n",
    "    \n",
    "    if len(lengths)==0: # no text detected in region\n",
    "        return [],[] # return empty and try again\n",
    "    \n",
    "    # this needs to be redone to check content of string, not just position\n",
    "    distsq = [ (r[0][0][0]-pos0[0])**2 + (r[0][0][1]-pos0[1])**2 for r in result ]\n",
    "    #idx = np.argmax( np.array(lengths) )\n",
    "    idx = np.argmin( np.array(distsq) )\n",
    "    string = result[idx][1]\n",
    "    coord = result[idx][0]\n",
    "    rect_txt = easyocr_coord_to_rect( coord )\n",
    "    img_bw = blackout_rectangle_exterior( img_bw, rect_txt)\n",
    "    text = pytesseract.image_to_string(img_bw)\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    return rect_txt, text\n",
    "\n",
    "\n",
    "\n",
    "### finish this function\n",
    "### 1. return coordinates with respect to original rectangle\n",
    "### 2. implement multiple methods \n",
    "def extract_text_from_rectangle(img, rect = (22,500,811,220), method = 'easyocr', thresh = 170, solidify = True):\n",
    "    # img is HxW or HxWx3 image in numpy array\n",
    "    # rect = (x0,y0,w,h) is region\n",
    "    # returns string and bounding rectangle with respect to original image\n",
    "    (x,y,w,h) = rect\n",
    "    img_cropped = img[y:y+h,x:x+w]\n",
    "    # use the reader from the global scope, bad coding, refactor later\n",
    "    result = reader.readtext(img_cropped,paragraph=True)\n",
    "    if len(result)>0:\n",
    "        size_strings = result[0][1]\n",
    "        coords = result[0][0]\n",
    "        # rectangle coord relative to input rect\n",
    "        rect1 = easyocr_coord_to_rect(coords)\n",
    "        rect_text = (rect[0]+rect1[0], rect[1]+rect1[1],rect1[2],rect1[3])\n",
    "    else:\n",
    "        size_strings = []\n",
    "        rect_text = []\n",
    "    \n",
    "    return size_strings, rect_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9c0ac75e-9ff3-4489-bfd4-2075836e595a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = None\n",
    "x == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcea56cf-3773-41aa-8eb4-75f3a5edfdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20 50 30\n"
     ]
    }
   ],
   "source": [
    "rect = (10,20,50,30)\n",
    "x,y,w,h = rect\n",
    "print(x,y,w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d444d7-8773-4c68-8b55-de838f99e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_box_extracter(img):\n",
    "    # img is HxW or HxWx3 image in numpy array\n",
    "    # returns string and bounding rectangle with respect to original image\n",
    "    \n",
    "    # start by looking for the last row of text in the size box by using easyocr\n",
    "    x = 722\n",
    "    y = 688\n",
    "    w = 900-x\n",
    "    h = 720-y\n",
    "    img_cropped = img[y:y+h,x:x+w]\n",
    "    # use the reader from the global scope, bad coding, refactor later\n",
    "    result = reader.readtext(img_cropped,paragraph=True)\n",
    "    \n",
    "    if len(result)>0:\n",
    "        size_string = result[0][1]\n",
    "        if len(size_string)<3: # it detected something but not a full size row\n",
    "            size_string = []\n",
    "            rect_box = []\n",
    "        else:\n",
    "            # print('Size String in Extraction: ',size_string)\n",
    "            first_char = size_string[0]\n",
    "            if not first_char.isnumeric():\n",
    "                first_char = '2' # this is a hack\n",
    "            num_rows = int(first_char)\n",
    "            h = 34*num_rows+26+5 # approx height of box\n",
    "            y = 720 - h # approximate top of box\n",
    "            x = 720 # tie this to the coordinate extraction\n",
    "            w = 900-x\n",
    "            #print('Inside size_box_extracter init guess: ',(x,y,w,h))\n",
    "            rect_box,size_string = get_box_rect(img, rect = (x,y,w,h), thresh = 40)\n",
    "            #print('Inside size_box_extracter rect_box: ',rect_box)\n",
    "    else:\n",
    "        size_string = []\n",
    "        rect_box = []\n",
    "    \n",
    "    return rect_box, size_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_1-12-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 22:25:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d7db18dfb91435869d7c8bf116ee1b09e20e7298c997c98856efe83d2801d70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
